experiment:
  name: "EgoLanesLite"    # name of the experiment, used for logging and checkpointing
  output_dir: "runs/training/EgoLanesLite/"
  seed: 42
  device: "cuda"    # "cuda", "cuda:0", or "cpu"
  wandb:
    enabled: true
    project_name: "EgoLanesLite"     #it is fixed from the account

checkpoint:
  load_from:         # path to checkpoint to load before training
  strict_load: true      # whether to strictly enforce that the keys in state_dict match the keys returned by the model's state_dict function

dataset:
  training_sets: ["tusimple", "curvelanes"]    # <--- LIST OF DATASETS TO USE FOR TRAIN
  acdc_root: "/home/sergey/DEV/AI/datasets/acdc"
  mapillary_root: "/home/sergey/DEV/AI/datasets/mapillary"
  muses_root: "/home/sergey/DEV/AI/datasets/MUSES"
  idda_root: "/home/sergey/DEV/AI/datasets/idda"
  bdd100k_root: "/home/sergey/DEV/AI/datasets/BDD100K"
  cityscapes_root: "/home/sergey/DEV/AI/datasets/cityscapes"
  tusimple_root: "/home/sergey/DEV/AI/datasets/TUSimple"
  curvelanes_root: "/home/sergey/DEV/AI/datasets/Curvelanes"

  validation_sets: ["tusimple", "curvelanes"]    # <--- LIST OF DATASETS TO USE FOR VALIDATION

  #augmentations settings for all datasets
  augmentations:
    normalize:
      enabled: true
      mean: [0.485, 0.456, 0.406]   # imagenet mean, RGB ORDER  
      std: [0.229, 0.224, 0.225]    # imagenet std, RGB ORDER

    rescaling:
      enabled: true
      mode: "fixed_resize"    # fixed_resize | random_crop

      #for random cropping
      scale_range: [1.0, 1.0]   # min and max scale for random cropping

      width: 800
      height: 400

    # Horizontal flip
    flip_prob: 0.5

    # Optional random grid shuffle
    random_grid_shuffle:
      enabled: false        # set true if you want it
      grid: [1, 2]
      prob: 0.25

    # Noise settings
    noise:
      profile: "none"   # none | moderate | heavy | roadwork
      prob: 0.5             # probability noise is applied


dataloader:
  batch_size: 8            # how many samples per optimization step
  num_workers: 2            # how many workers for data loading
  pin_memory: true          # pin memory for dataloader
  persistent_workers: true  # keep workers alive between epochs
  drop_last: true           # drop last incomplete batch
  prefetch_factor: 2        # number of batches to prefetch
  shuffle_train: true       # shuffle training data (random order, but complete every sample before epoch)
  shuffle_val: false        # do not shuffle validation data

training:
  mode: "steps"        # "epoch" or "steps" 
  max_epochs: 40       # used if mode == "epoch"
  max_steps: 150000     # used if mode == "steps". step = optimization step (so affected by batch size and grad accum)
  grad_accum_steps: 1

  validation:
    mode: "steps"        # "epoch" or "steps"
    every_n_epochs: 1    # used if mode == "epoch"
    every_n_steps: 2000    # used if mode == "steps"

  logging:
    log_every_steps: 50    # step = optimization step (so affected by batch size and grad accum)
  
  save_best: true           # save best mIoU
  save_last: true           # always save last


optimizer:
  type: "adamw"
  lr: 1.0e-4              #base learning rate

  #adamw parameters
  weight_decay: 1.0e-2    #default
  betas: [0.9, 0.999]     #default

scheduler:
  type: "warmup_cosine"              # "none" or "step", "cosine", "warmup_cosine", "poly"
  
  #for "step" scheduler 
  step_size: 30           # used if type == "step"
  gamma: 0.1              # used if type == "step"
  
  #for "warmup_cosine" scheduler and "cosine" scheduler
  warmup_steps: 1000      # used if type == "warmup_cosine". Note: steps = OPTIMIZATION STEPS (batch_size * iterations)
  min_lr: 5.0e-6          # used if type == "warmup_cosine"

loss:

network:
  model: "deeplabv3plus"    #deeplabv3plus | fcn | unetplusplus
  type: "custom"    #standard uses the smp.deeplabv3plus architecture, custom uses my own integrated implementation (used for loading some weights customly)
  

  #load a pretrained model to use as backbone (also as decoder if specified)
  pretrained_model_path:  #"/home/sergey/DEV/AI/AEI/runs/training/segmentation/dlv3plus_efficientnetb1_v2/checkpoints/best_mIoU.pth"     # path to a pretrained model to use as backbone (overrides 'pretrained' flag)

  backbone:
    type: "efficientnet_b1"  # e.g., "resnet50", "efficientnet_b0", etc.
    pretrained: true         # whether to use ImageNet pre-trained weights
    encoder_depth: 5      #the encoder features that will be used in decoder (5 means the last stage before 1x1conv, 4 means the one before that, etc.)

    encoder_partial_load: false      # whether to partially load encoder weights from the pretrained model specified in pretrained_model_path
    encoder_partial_depth: 4        # how many stages to load if encoder_partial_load is
    
    output_stride: 16        # 8 or 16

    #if pretrained model is used. Ignore the type and pretrained flags
    load_encoder: false             # whether to load encoder weights from the pretrained model specified in pretrained_model_path
    freeze_encoder: false            # whether to freeze backbone weights during training. It freezes until the encoder_partial_depth if encoder_partial_load is true

  bottleneck:
    #used to better map the backbone features to the decoder input
    type : "none"   # "none" | "fcn" | "fcn_cbam" | "fcn_skip" | "fcn_skip_cbam"
    #number of channels is the same of the backbone output channels

  decoder:
    #for deeplabv3plus
    aspp_dilations: [12, 24, 36]   #dilation rates for ASPP module
    deeplabv3plus_decoder_channels: 64        #number of channels in decoder

    load_decoder: false    # whether to load decoder weights from the pretrained model specified in backbone.pretrained_model_path
    freeze_decoder: false    # whether to freeze decoder weights during training

  head:
    head_activation: null
    head_depth: 1
    head_mid_channels: None
    head_upsampling: 1
    head_kernel_size: 3

  output_channels: 3